{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Age', 'Gender', 'Physical', 'Race', 'Religion', 'Others']\n",
    "id2label = {idx:label for idx, label in enumerate(LABELS)}\n",
    "label2id = {label:idx for idx, label in enumerate(LABELS)}\n",
    "\n",
    "text = \"Putanginang bata to feeling amerikano\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(model.device) for k,v in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the input tensors\n",
    "input_shape = encoding[\"input_ids\"].shape\n",
    "print(f\"Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 8])\n",
      "token_type_ids: torch.Size([1, 8])\n",
      "attention_mask: torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "for key, tensor in encoding.items():\n",
    "    print(f\"{key}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = {k: torch.zeros_like(v) for k, v in encoding.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Race']\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**encoding)\n",
    "output = outputs.logits\n",
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(output.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in /home/syke/.local/lib/python3.10/site-packages (1.16.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/syke/.local/lib/python3.10/site-packages (from onnx) (5.27.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/syke/.local/lib/python3.10/site-packages (from onnx) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnxscript\n",
      "  Downloading onnxscript-0.1.0.dev20240717-py3-none-any.whl (644 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.3/644.3 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/syke/.local/lib/python3.10/site-packages (from onnxscript) (4.12.2)\n",
      "Requirement already satisfied: onnx>=1.16 in /home/syke/.local/lib/python3.10/site-packages (from onnxscript) (1.16.1)\n",
      "Collecting ml-dtypes\n",
      "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/syke/.local/lib/python3.10/site-packages (from onnxscript) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/syke/.local/lib/python3.10/site-packages (from onnx>=1.16->onnxscript) (5.27.2)\n",
      "Installing collected packages: ml-dtypes, onnxscript\n",
      "Successfully installed ml-dtypes-0.4.0 onnxscript-0.1.0.dev20240717\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install onnx\n",
    "%pip install onnxscript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Get the input shape for the ONNX conversion\n",
    "input_shape = encoding['input_ids'].shape\n",
    "print(f\"Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX format\n",
    "dummy_input = (torch.ones(input_shape).long().to(model.device),) # assuming only input_ids is used\n",
    "onnx_model_path = \"bert_multilabel_tagalog_hate_speech_classifier.onnx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,  # model input (or a tuple for multiple inputs)\n",
    "    onnx_model_path,  # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,  # store the trained parameter weights inside the model file\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=['input_ids'],  # the model's input names\n",
    "    output_names=['logits'],  # the model's output names\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}, 'logits': {0: 'batch_size'}}  # variable length axes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"bert_multilabel_tagalog_hate_speech_classifier.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the example text\n",
    "encoding = tokenizer(text, return_tensors=\"np\")\n",
    "\n",
    "# Prepare the input data for ONNX runtime\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 1. 0. 0.]\n",
      "['Age', 'Race']\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: input_ids}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Get the output logits\n",
    "output_logits = ort_outs[0]\n",
    "\n",
    "# Apply sigmoid + threshold\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "probs = sigmoid(output_logits.squeeze())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Turn predicted ids into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m scripts.convert --quantize --model_id syke9p3/bert-multilabel-tagalog-hate-speech-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"bert_multilabel_tagalog_hate_speech_classifier.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "/home/syke/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "\n",
      "***** Exporting submodel 1/1: BertForSequenceClassification *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "\n",
    "onnx_model = ORTModelForSequenceClassification.from_pretrained(\"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\",from_transformers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path = \"onnx\"\n",
    "\n",
    "onnx_model.save_pretrained(\"onnx\")\n",
    "tokenizer.save_pretrained(\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load the model from syke9p3/bert-multilabel-tagalog-hate-speech-classifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ORTOptimizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizationConfig\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mORTOptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msyke9p3/bert-multilabel-tagalog-hate-speech-classifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m optimization_config \u001b[38;5;241m=\u001b[39m OptimizationConfig(optimization_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99\u001b[39m)\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mexport(\n\u001b[1;32m      8\u001b[0m     onnx_model_path\u001b[38;5;241m=\u001b[39monnx_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     onnx_optimized_model_output_path\u001b[38;5;241m=\u001b[39monnx_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-optimized.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     optimization_config\u001b[38;5;241m=\u001b[39moptimization_config,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optimum/onnxruntime/optimization.py:118\u001b[0m, in \u001b[0;36mORTOptimizer.from_pretrained\u001b[0;34m(cls, model_or_path, file_names)\u001b[0m\n\u001b[1;32m    116\u001b[0m         onnx_model_path\u001b[38;5;241m.\u001b[39mappend(model_or_path\u001b[38;5;241m.\u001b[39mjoinpath(file_name))\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load the model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(onnx_model_path, config\u001b[38;5;241m=\u001b[39mconfig, from_ortmodel\u001b[38;5;241m=\u001b[39mfrom_ortmodel)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to load the model from syke9p3/bert-multilabel-tagalog-hate-speech-classifier."
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "optimizer = ORTOptimizer.from_pretrained(\"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\")\n",
    "optimization_config = OptimizationConfig(optimization_level=99)\n",
    "\n",
    "optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_optimized_model_output_path=onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
