{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syke/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "model_name = \"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "/home/syke/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "\n",
      "***** Exporting submodel 1/1: BertForSequenceClassification *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\", from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"syke9p3/bert-multilabel-tagalog-hate-speech-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Religion', 'score': 0.8994330167770386},\n",
       "  {'label': 'Race', 'score': 0.8859398365020752},\n",
       "  {'label': 'Age', 'score': 0.04573800414800644},\n",
       "  {'label': 'Physical', 'score': 0.04536525532603264},\n",
       "  {'label': 'Gender', 'score': 0.03270665556192398},\n",
       "  {'label': 'Others', 'score': 0.006826996803283691}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, top_k=None)\n",
    "classifier(\"Parang gago naman tong katolikong arabo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syke/.local/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Optimizing model...\n",
      "\u001b[0;93m2024-07-17 15:02:59.811315473 [W:onnxruntime:, inference_session.cc:1978 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n",
      "Configuration saved in onnx/ort_config.json\n",
      "Optimized model saved at: onnx (external data format: False; saved all tensor to one file: True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    " \n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(\n",
    "    save_dir=onnx_path,\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Religion', 'score': 0.8994330167770386},\n",
       "  {'label': 'Race', 'score': 0.88593989610672},\n",
       "  {'label': 'Age', 'score': 0.045737992972135544},\n",
       "  {'label': 'Physical', 'score': 0.04536525532603264},\n",
       "  {'label': 'Gender', 'score': 0.03270664066076279},\n",
       "  {'label': 'Others', 'score': 0.006826999597251415}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load optimized model\n",
    "optimized_model = ORTModelForSequenceClassification.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
    " \n",
    "# create optimized pipeline\n",
    "optimized_clf = pipeline(\"text-classification\", model=optimized_model, tokenizer=tokenizer, top_k=None)\n",
    "optimized_clf(\"Parang gago naman tong katolikong arabo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/u8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: onnx (external data format: False)\n",
      "Configuration saved in onnx/ort_config.json\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "dynamic_quantizer = ORTQuantizer.from_pretrained(model)\n",
    "dqconfig = AutoQuantizationConfig.avx2(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "model_quantized_path = dynamic_quantizer.quantize(\n",
    "    save_dir=onnx_path,\n",
    "    quantization_config=dqconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 481.00 MB\n",
      "Quantized Model file size: 121.10 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model_optimized.onnx\")/(1024*1024)\n",
    "quantized_model = os.path.getsize(onnx_path / \"model_quantized.onnx\")/(1024*1024)\n",
    "\n",
    "print(f\"Model file size: {size:.2f} MB\")\n",
    "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized = ORTModelForSequenceClassification.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "\n",
    "q8_clf = pipeline(\"text-classification\",model=model_quantized, tokenizer=tokenizer, top_k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Gender', 'score': 0.9606606364250183},\n",
       "  {'label': 'Race', 'score': 0.6761333346366882},\n",
       "  {'label': 'Religion', 'score': 0.08153875172138214},\n",
       "  {'label': 'Physical', 'score': 0.044747352600097656},\n",
       "  {'label': 'Age', 'score': 0.033716924488544464},\n",
       "  {'label': 'Others', 'score': 0.011751459911465645}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q8_clf(\"Putangina naman netong mga arabong bakla\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
